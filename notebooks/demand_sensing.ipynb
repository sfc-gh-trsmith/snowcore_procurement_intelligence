{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d083925b",
      "metadata": {
        "name": "title_and_objectives"
      },
      "source": [
        "# Snowcore Industries - Demand Sensing Model\n",
        "\n",
        "## Business Objective\n",
        "\n",
        "Snowcore Industries requires accurate demand forecasting to optimize procurement planning and reduce inventory costs. This notebook implements a multi-model demand sensing system that predicts `forecasted_material_demand_qty` by combining:\n",
        "- Historical consumption patterns\n",
        "- External macro-economic indicators (construction starts, clinical trial spend, PMI)\n",
        "- Seasonal patterns\n",
        "\n",
        "## Technical Approach\n",
        "\n",
        "We employ an **ensemble comparison approach** using three regression algorithms:\n",
        "1. **XGBoost** (primary) - Gradient boosted decision trees optimized for structured data\n",
        "2. **Random Forest** - Ensemble of decision trees using bagging\n",
        "3. **Linear Regression** - Simple baseline for comparison\n",
        "\n",
        "The best model is selected based on Mean Absolute Percentage Error (MAPE) and deployed for production forecasting.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "After completing this notebook, you will understand:\n",
        "1. How to engineer features from historical demand data and external indicators\n",
        "2. The differences between XGBoost, Random Forest, and Linear Regression for demand forecasting\n",
        "3. How to evaluate regression models using MAE, RMSE, MAPE, and R²\n",
        "4. Best practices for temporal train/test splits to prevent data leakage\n",
        "5. How to generate backtests and future forecasts with confidence intervals\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- **Mathematics**: Basic statistics (mean, standard deviation, correlation)\n",
        "- **ML Concepts**: Supervised learning, regression, train/test splits, cross-validation\n",
        "- **Python**: Pandas, basic SQL, Snowpark familiarity\n",
        "- **Domain**: Understanding of supply chain/procurement planning (helpful but not required)\n",
        "\n",
        "## Notebook Structure\n",
        "\n",
        "| Section | Purpose |\n",
        "|---------|---------|\n",
        "| 1. Environment Setup | Session configuration, imports, visualization theme |\n",
        "| 2. Data Exploration | Load demand data, explore distributions |\n",
        "| 3. Feature Engineering | Create historical, external, and seasonal features |\n",
        "| 4. Algorithm Overview | Conceptual explanation of XGBoost, Random Forest, Linear Regression |\n",
        "| 5. Model Training | Train all three models on training data |\n",
        "| 6. Evaluation | Compare model performance with metrics and visualizations |\n",
        "| 7. Production Output | Save predictions, model registry, feature importance |\n",
        "| 8. Key Takeaways | Summary, interpretation guidelines, limitations |\n",
        "\n",
        "## Output Artifacts\n",
        "\n",
        "| Table | Description |\n",
        "|-------|-------------|\n",
        "| `ATOMIC.DEMAND_FORECAST_PREDICTIONS` | 90-day forecasts + 6-month backtests with confidence intervals |\n",
        "| `ATOMIC.MODEL_REGISTRY` | Model performance metrics for tracking |\n",
        "| `ATOMIC.MODEL_FEATURE_IMPORTANCE` | Feature importance scores from XGBoost |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1b99658",
      "metadata": {
        "language": "python",
        "name": "environment_setup_cell"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ENVIRONMENT SETUP\n",
        "# =============================================================================\n",
        "# Configure Snowflake session, imports, visualization theme, and helper functions\n",
        "\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "session = get_active_session()\n",
        "\n",
        "# Import required libraries\n",
        "from snowflake.snowpark import functions as F\n",
        "from snowflake.snowpark.types import *\n",
        "from snowflake.ml.modeling.xgboost import XGBRegressor\n",
        "from snowflake.ml.modeling.ensemble import RandomForestRegressor\n",
        "from snowflake.ml.modeling.linear_model import LinearRegression\n",
        "from snowflake.ml.modeling.preprocessing import StandardScaler\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# =============================================================================\n",
        "# DARK THEME VISUALIZATION SETUP (Snowflake-inspired)\n",
        "# =============================================================================\n",
        "# Configure matplotlib for dark backgrounds with colorblind-safe palette\n",
        "\n",
        "plt.style.use('dark_background')\n",
        "plt.rcParams.update({\n",
        "    # Background colors (soft dark gray, not pure black)\n",
        "    'figure.facecolor': '#121212',\n",
        "    'axes.facecolor': '#121212',\n",
        "    \n",
        "    # Text colors (off-white to reduce glare)\n",
        "    'text.color': '#E5E5E7',\n",
        "    'axes.labelcolor': '#E5E5E7',\n",
        "    'xtick.color': '#A1A1A6',\n",
        "    'ytick.color': '#A1A1A6',\n",
        "    \n",
        "    # Grid and axes (subtle, not distracting)\n",
        "    'axes.edgecolor': '#3A3A3C',\n",
        "    'grid.color': '#2C2C2E',\n",
        "    'grid.alpha': 0.6,\n",
        "    \n",
        "    # Line and marker styling\n",
        "    'lines.linewidth': 2,\n",
        "    'lines.markersize': 8,\n",
        "    \n",
        "    # Figure quality and sizing\n",
        "    'figure.dpi': 150,\n",
        "    'savefig.dpi': 200,\n",
        "    'figure.figsize': (10, 6),\n",
        "    \n",
        "    # Font configuration\n",
        "    'font.family': 'sans-serif',\n",
        "    'font.size': 11,\n",
        "    'axes.titlesize': 14,\n",
        "    'axes.labelsize': 12,\n",
        "})\n",
        "\n",
        "# Colorblind-safe desaturated palette for dark backgrounds\n",
        "COLORS = ['#64D2FF', '#FF9F0A', '#5AC8FA', '#FFD60A', '#11567F']\n",
        "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=COLORS)\n",
        "\n",
        "# =============================================================================\n",
        "# FAIL-FAST ERROR HANDLING\n",
        "# =============================================================================\n",
        "# All queries must fail loudly - no silent failures\n",
        "\n",
        "def execute_query(session, query: str, name: str = \"query\"):\n",
        "    \"\"\"\n",
        "    Execute a SQL query with fail-fast error handling.\n",
        "    \n",
        "    Args:\n",
        "        session: Snowflake Snowpark session\n",
        "        query: SQL query string\n",
        "        name: Descriptive name for error messages\n",
        "    \n",
        "    Returns:\n",
        "        Snowpark DataFrame with query results\n",
        "    \n",
        "    Raises:\n",
        "        RuntimeError: If query fails or returns None\n",
        "    \"\"\"\n",
        "    try:\n",
        "        result = session.sql(query)\n",
        "        if result is None:\n",
        "            raise RuntimeError(f\"Query '{name}' returned None\")\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Query '{name}' failed: {e}\") from e\n",
        "\n",
        "print(f\"Connected to: {session.get_current_database()}.{session.get_current_schema()}\")\n",
        "print(f\"Visualization theme: Dark mode with colorblind-safe palette\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f77f81aa",
      "metadata": {
        "name": "data_exploration_header"
      },
      "source": [
        "## 2. Data Exploration\n",
        "\n",
        "Load and explore demand data to understand distributions, patterns, and data quality before feature engineering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bf6c894",
      "metadata": {
        "language": "python",
        "name": "data_exploration_cell"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DATA EXPLORATION\n",
        "# =============================================================================\n",
        "# Load demand data and explore key statistics\n",
        "\n",
        "demand_df = session.table(\"ATOMIC.DEMAND_ACTUAL\")\n",
        "record_count = demand_df.count()\n",
        "print(f\"Total demand records: {record_count:,}\")\n",
        "\n",
        "if record_count == 0:\n",
        "    raise RuntimeError(\"DEMAND_ACTUAL table is empty - cannot proceed with analysis\")\n",
        "\n",
        "# Date range\n",
        "print(\"\\n=== Date Range ===\")\n",
        "demand_df.select(\n",
        "    F.min(\"DEMAND_DATE\").alias(\"MIN_DATE\"),\n",
        "    F.max(\"DEMAND_DATE\").alias(\"MAX_DATE\")\n",
        ").show()\n",
        "\n",
        "# Demand by source\n",
        "print(\"\\n=== Demand by Source ===\")\n",
        "demand_df.group_by(\"DEMAND_SOURCE\").agg(\n",
        "    F.count(\"*\").alias(\"COUNT\"),\n",
        "    F.sum(\"ACTUAL_QUANTITY\").alias(\"TOTAL_QTY\")\n",
        ").order_by(\"TOTAL_QTY\", ascending=False).show()\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\n=== Demand Quantity Statistics ===\")\n",
        "demand_df.select(\n",
        "    F.avg(\"ACTUAL_QUANTITY\").alias(\"AVG_QTY\"),\n",
        "    F.stddev(\"ACTUAL_QUANTITY\").alias(\"STD_QTY\"),\n",
        "    F.min(\"ACTUAL_QUANTITY\").alias(\"MIN_QTY\"),\n",
        "    F.max(\"ACTUAL_QUANTITY\").alias(\"MAX_QTY\"),\n",
        "    F.percentile_cont(0.5).within_group(\"ACTUAL_QUANTITY\").alias(\"MEDIAN_QTY\")\n",
        ").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5df49df",
      "metadata": {
        "language": "python",
        "name": "data_visualization_cell"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DATA EXPLORATION VISUALIZATIONS\n",
        "# =============================================================================\n",
        "# Visualize demand distributions and patterns\n",
        "\n",
        "# Get data for plotting\n",
        "demand_stats = demand_df.group_by(\"DEMAND_SOURCE\").agg(\n",
        "    F.count(\"*\").alias(\"COUNT\"),\n",
        "    F.avg(\"ACTUAL_QUANTITY\").alias(\"AVG_QTY\"),\n",
        "    F.stddev(\"ACTUAL_QUANTITY\").alias(\"STD_QTY\")\n",
        ").to_pandas()\n",
        "\n",
        "# Monthly demand trends\n",
        "monthly_demand = demand_df.select(\n",
        "    F.date_trunc(\"month\", F.col(\"DEMAND_DATE\")).alias(\"MONTH\"),\n",
        "    F.col(\"ACTUAL_QUANTITY\")\n",
        ").group_by(\"MONTH\").agg(\n",
        "    F.sum(\"ACTUAL_QUANTITY\").alias(\"TOTAL_QTY\"),\n",
        "    F.count(\"*\").alias(\"ORDER_COUNT\")\n",
        ").order_by(\"MONTH\").to_pandas()\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
        "\n",
        "# Plot 1: Demand by Source\n",
        "ax1 = axes[0]\n",
        "bars = ax1.barh(demand_stats['DEMAND_SOURCE'], demand_stats['COUNT'], color=COLORS[0])\n",
        "ax1.set_xlabel('Number of Records')\n",
        "ax1.set_title('Demand Records by Source')\n",
        "ax1.grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Plot 2: Monthly Demand Trend\n",
        "ax2 = axes[1]\n",
        "if len(monthly_demand) > 0:\n",
        "    ax2.plot(monthly_demand['MONTH'], monthly_demand['TOTAL_QTY'], \n",
        "             color=COLORS[0], marker='o', markersize=4)\n",
        "    ax2.fill_between(monthly_demand['MONTH'], monthly_demand['TOTAL_QTY'], \n",
        "                     alpha=0.3, color=COLORS[0])\n",
        "ax2.set_xlabel('Month')\n",
        "ax2.set_ylabel('Total Quantity')\n",
        "ax2.set_title('Monthly Demand Trend')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "# Plot 3: Average Demand by Source\n",
        "ax3 = axes[2]\n",
        "bars = ax3.barh(demand_stats['DEMAND_SOURCE'], demand_stats['AVG_QTY'], \n",
        "                color=COLORS[1], xerr=demand_stats['STD_QTY'].fillna(0), \n",
        "                capsize=3, error_kw={'color': '#A1A1A6', 'alpha': 0.7})\n",
        "ax3.set_xlabel('Average Quantity (with Std Dev)')\n",
        "ax3.set_title('Average Demand by Source')\n",
        "ax3.grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/tmp/demand_exploration.png', dpi=150, bbox_inches='tight', \n",
        "            facecolor='#121212', edgecolor='none')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nData exploration visualizations saved to /tmp/demand_exploration.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c84c400e",
      "metadata": {
        "name": "feature_engineering_header"
      },
      "source": [
        "## 3. Feature Engineering\n",
        "\n",
        "Create features from three sources:\n",
        "1. **Internal features**: Historical consumption patterns (average, variability)\n",
        "2. **External features**: Macro-economic indicators from marketplace data\n",
        "3. **Seasonal features**: Time-based patterns (day of week, month, quarter)\n",
        "\n",
        "> **Note**: All features are cast to FLOAT for XGBoost compatibility (INTEGER maps to object type in Snowpark ML)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b32ddca5",
      "metadata": {
        "language": "python",
        "name": "feature_engineering_sql_cell"
      },
      "outputs": [],
      "source": [
        "# Feature engineering SQL with external indicators\n",
        "# NOTE: All features must be FLOAT for XGBoost compatibility (INTEGER maps to object type)\n",
        "feature_sql = \"\"\"\n",
        "WITH demand_stats AS (\n",
        "    SELECT \n",
        "        PRODUCT_ID,\n",
        "        SITE_ID,\n",
        "        AVG(ACTUAL_QUANTITY) AS AVG_QTY,\n",
        "        STDDEV(ACTUAL_QUANTITY) AS STD_QTY,\n",
        "        COUNT(*) AS RECORD_COUNT\n",
        "    FROM ATOMIC.DEMAND_ACTUAL\n",
        "    GROUP BY PRODUCT_ID, SITE_ID\n",
        "),\n",
        "-- Get external indicators (pivoted)\n",
        "external_indicators AS (\n",
        "    SELECT \n",
        "        DATE_TRUNC('week', INDICATOR_DATE) AS WEEK_DATE,\n",
        "        AVG(CASE WHEN INDICATOR_NAME = 'Construction Starts Index' THEN INDICATOR_VALUE END) AS CONSTRUCTION_IDX,\n",
        "        AVG(CASE WHEN INDICATOR_NAME = 'Clinical Trial Spend ($B)' THEN INDICATOR_VALUE END) AS CLINICAL_IDX,\n",
        "        AVG(CASE WHEN INDICATOR_NAME = 'Industrial Production Index' THEN INDICATOR_VALUE END) AS PRODUCTION_IDX,\n",
        "        AVG(CASE WHEN INDICATOR_NAME = 'Manufacturing PMI' THEN INDICATOR_VALUE END) AS PMI_IDX\n",
        "    FROM ATOMIC.MARKETPLACE_INDICATORS\n",
        "    GROUP BY DATE_TRUNC('week', INDICATOR_DATE)\n",
        ")\n",
        "SELECT \n",
        "    d.DEMAND_ACTUAL_ID,\n",
        "    d.PRODUCT_ID,\n",
        "    d.SITE_ID,\n",
        "    d.DEMAND_DATE,\n",
        "    d.ACTUAL_QUANTITY,\n",
        "    -- Internal features (all FLOAT for XGBoost)\n",
        "    CAST(COALESCE(ds.AVG_QTY, 100) AS FLOAT) AS HIST_AVG_QTY,\n",
        "    CAST(COALESCE(ds.STD_QTY, 10) AS FLOAT) AS HIST_STD_QTY,\n",
        "    CAST(COALESCE(p.PRODUCT_CATEGORY_ID, 1) AS FLOAT) AS PRODUCT_CATEGORY_ID,\n",
        "    -- External features\n",
        "    CAST(COALESCE(ei.CONSTRUCTION_IDX, 100) AS FLOAT) AS CONSTRUCTION_IDX,\n",
        "    CAST(COALESCE(ei.CLINICAL_IDX, 2.5) AS FLOAT) AS CLINICAL_IDX,\n",
        "    CAST(COALESCE(ei.PRODUCTION_IDX, 98) AS FLOAT) AS PRODUCTION_IDX,\n",
        "    CAST(COALESCE(ei.PMI_IDX, 50) AS FLOAT) AS PMI_IDX,\n",
        "    -- Seasonal features (all FLOAT for XGBoost)\n",
        "    CAST(DAYOFWEEK(d.DEMAND_DATE) AS FLOAT) AS DAY_OF_WEEK,\n",
        "    CAST(MONTH(d.DEMAND_DATE) AS FLOAT) AS MONTH_NUM,\n",
        "    CAST(QUARTER(d.DEMAND_DATE) AS FLOAT) AS QUARTER_NUM,\n",
        "    -- Target\n",
        "    CAST(d.ACTUAL_QUANTITY AS FLOAT) AS TARGET\n",
        "FROM ATOMIC.DEMAND_ACTUAL d\n",
        "LEFT JOIN demand_stats ds ON d.PRODUCT_ID = ds.PRODUCT_ID AND d.SITE_ID = ds.SITE_ID\n",
        "LEFT JOIN ATOMIC.PRODUCT p ON d.PRODUCT_ID = p.PRODUCT_ID\n",
        "LEFT JOIN external_indicators ei ON DATE_TRUNC('week', d.DEMAND_DATE) = ei.WEEK_DATE\n",
        "WHERE ds.RECORD_COUNT >= 2\n",
        "\"\"\"\n",
        "\n",
        "features_df = session.sql(feature_sql)\n",
        "print(f\"Feature dataset rows: {features_df.count():,}\")\n",
        "features_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a000e484",
      "metadata": {
        "language": "python",
        "name": "feature_definition_cell"
      },
      "outputs": [],
      "source": [
        "# Define features and target\n",
        "# Internal features: historical consumption patterns\n",
        "# External features: macro-economic indicators\n",
        "# Seasonal features: time-based patterns\n",
        "feature_cols = [\n",
        "    # Internal\n",
        "    'HIST_AVG_QTY', 'HIST_STD_QTY', 'PRODUCT_CATEGORY_ID',\n",
        "    # External macro-economic\n",
        "    'CONSTRUCTION_IDX', 'CLINICAL_IDX', 'PRODUCTION_IDX', 'PMI_IDX',\n",
        "    # Seasonal\n",
        "    'DAY_OF_WEEK', 'MONTH_NUM', 'QUARTER_NUM'\n",
        "]\n",
        "target_col = 'TARGET'\n",
        "\n",
        "print(f\"Feature count: {len(feature_cols)}\")\n",
        "print(f\"Features: {feature_cols}\")\n",
        "\n",
        "# Train/test split (temporal - 80/20)\n",
        "date_stats = features_df.select(\n",
        "    F.min(\"DEMAND_DATE\").alias(\"MIN_DATE\"),\n",
        "    F.max(\"DEMAND_DATE\").alias(\"MAX_DATE\")\n",
        ").collect()[0]\n",
        "min_date = date_stats[\"MIN_DATE\"]\n",
        "max_date = date_stats[\"MAX_DATE\"]\n",
        "\n",
        "# Calculate 80% point as days from min\n",
        "total_days = (max_date - min_date).days\n",
        "split_days = int(total_days * 0.8)\n",
        "split_date = min_date + datetime.timedelta(days=split_days)\n",
        "\n",
        "train_df = features_df.filter(F.col(\"DEMAND_DATE\") < F.lit(split_date))\n",
        "test_df = features_df.filter(F.col(\"DEMAND_DATE\") >= F.lit(split_date))\n",
        "\n",
        "print(f\"\\nSplit date: {split_date}\")\n",
        "print(f\"Training samples: {train_df.count():,}\")\n",
        "print(f\"Testing samples: {test_df.count():,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05d51372",
      "metadata": {
        "name": "algorithm_explainer"
      },
      "source": [
        "## 4. Algorithm Overview\n",
        "\n",
        "Before training, let's understand the three regression algorithms we'll compare.\n",
        "\n",
        "### What is XGBoost?\n",
        "\n",
        "**XGBoost (Extreme Gradient Boosting)** is an optimized gradient boosting algorithm that builds an ensemble of decision trees sequentially. Each tree corrects the errors of the previous trees.\n",
        "\n",
        "**Key intuition**: Instead of building one complex tree, XGBoost builds many simple trees where each tree focuses on the mistakes of the previous ones.\n",
        "\n",
        "**Mathematical formulation**:\n",
        "$$\\hat{y}_i = \\sum_{k=1}^{K} f_k(x_i)$$\n",
        "\n",
        "Where $f_k$ is the prediction from tree $k$, and trees are added to minimize the loss function.\n",
        "\n",
        "### What is Random Forest?\n",
        "\n",
        "**Random Forest** is an ensemble of decision trees trained independently using bagging (bootstrap aggregating). Each tree sees a random subset of data and features, and predictions are averaged.\n",
        "\n",
        "**Key intuition**: \"Wisdom of crowds\" - averaging many diverse trees reduces overfitting and improves generalization.\n",
        "\n",
        "**Mathematical formulation**:\n",
        "$$\\hat{y} = \\frac{1}{K} \\sum_{k=1}^{K} T_k(x)$$\n",
        "\n",
        "Where $T_k$ is the prediction from tree $k$ trained on a bootstrap sample.\n",
        "\n",
        "### Why Multi-Model Comparison?\n",
        "\n",
        "| Model | Strengths | Weaknesses |\n",
        "|-------|-----------|------------|\n",
        "| **XGBoost** | Handles non-linear relationships, robust to outliers | Can overfit with many features |\n",
        "| **Random Forest** | Less prone to overfitting, parallelizable | May miss subtle patterns |\n",
        "| **Linear Regression** | Interpretable, fast, good baseline | Assumes linear relationships |\n",
        "\n",
        "### Key Hyperparameters\n",
        "\n",
        "| Parameter | XGBoost | Random Forest | Purpose |\n",
        "|-----------|---------|---------------|---------|\n",
        "| `n_estimators` | 100 | 100 | Number of trees in ensemble |\n",
        "| `max_depth` | 6 | 8 | Maximum tree depth (controls complexity) |\n",
        "| `learning_rate` | 0.1 | N/A | Step size for gradient updates |\n",
        "| `random_state` | 42 | 42 | Reproducibility seed |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a905761d",
      "metadata": {
        "name": "model_training_header"
      },
      "source": [
        "## 5. Model Training\\n\\nTrain multiple models and compare performance:\\n- **XGBoost Regressor** (primary model) - Gradient boosted trees\\n- **Random Forest Regressor** (ensemble baseline) - Bagged decision trees\\n- **Linear Regression** (simple baseline) - Linear model for comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb848e95",
      "metadata": {
        "language": "python",
        "name": "model_training_cell"
      },
      "outputs": [],
      "source": [
        "# Train multiple models for comparison\n",
        "models = {}\n",
        "\n",
        "# 1. XGBoost Regressor (Primary Model)\n",
        "print(\"Training XGBoost model...\")\n",
        "xgb_model = XGBRegressor(\n",
        "    input_cols=feature_cols,\n",
        "    label_cols=[target_col],\n",
        "    output_cols=[\"PREDICTION\"],\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "xgb_model.fit(train_df)\n",
        "models['XGBoost'] = xgb_model\n",
        "print(\"  XGBoost training complete!\")\n",
        "\n",
        "# 2. Random Forest Regressor (Ensemble Baseline)\n",
        "print(\"Training Random Forest model...\")\n",
        "rf_model = RandomForestRegressor(\n",
        "    input_cols=feature_cols,\n",
        "    label_cols=[target_col],\n",
        "    output_cols=[\"PREDICTION\"],\n",
        "    n_estimators=100,\n",
        "    max_depth=8,\n",
        "    random_state=42\n",
        ")\n",
        "rf_model.fit(train_df)\n",
        "models['RandomForest'] = rf_model\n",
        "print(\"  Random Forest training complete!\")\n",
        "\n",
        "# 3. Linear Regression (Simple Baseline)\n",
        "print(\"Training Linear Regression model...\")\n",
        "lr_model = LinearRegression(\n",
        "    input_cols=feature_cols,\n",
        "    label_cols=[target_col],\n",
        "    output_cols=[\"PREDICTION\"]\n",
        ")\n",
        "lr_model.fit(train_df)\n",
        "models['LinearRegression'] = lr_model\n",
        "print(\"  Linear Regression training complete!\")\n",
        "\n",
        "print(f\"\\nAll {len(models)} models trained successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07f73ba1",
      "metadata": {
        "language": "python",
        "name": "feature_importance_cell"
      },
      "outputs": [],
      "source": [
        "# Extract and save feature importance from XGBoost (primary model)\n",
        "feature_importance_data = []\n",
        "importances = xgb_model.to_xgboost().feature_importances_\n",
        "\n",
        "for i, feat in enumerate(feature_cols):\n",
        "    # Determine feature type based on naming conventions\n",
        "    if feat.startswith('HIST_'):\n",
        "        feat_type = 'Internal'\n",
        "    elif feat.endswith('_IDX'):\n",
        "        feat_type = 'External'\n",
        "    elif feat in ['DAY_OF_WEEK', 'MONTH_NUM', 'QUARTER_NUM']:\n",
        "        feat_type = 'Seasonal'\n",
        "    else:\n",
        "        feat_type = 'Derived'\n",
        "    \n",
        "    description = {\n",
        "        'HIST_AVG_QTY': 'Historical average demand quantity',\n",
        "        'HIST_STD_QTY': 'Historical demand variability',\n",
        "        'PRODUCT_CATEGORY_ID': 'Product category classification',\n",
        "        'CONSTRUCTION_IDX': 'Construction starts index (economic indicator)',\n",
        "        'CLINICAL_IDX': 'Clinical trial spend (industry indicator)',\n",
        "        'PRODUCTION_IDX': 'Industrial production index (economic indicator)',\n",
        "        'PMI_IDX': 'Manufacturing PMI (economic indicator)',\n",
        "        'DAY_OF_WEEK': 'Day of week seasonality',\n",
        "        'MONTH_NUM': 'Monthly seasonality',\n",
        "        'QUARTER_NUM': 'Quarterly seasonality'\n",
        "    }.get(feat, f'Feature: {feat}')\n",
        "    \n",
        "    feature_importance_data.append({\n",
        "        'MODEL_VERSION': 'v5.0.0',\n",
        "        'FEATURE_NAME': feat,\n",
        "        'IMPORTANCE_SCORE': float(importances[i]),\n",
        "        'FEATURE_TYPE': feat_type,\n",
        "        'DESCRIPTION': description\n",
        "    })\n",
        "\n",
        "# Sort by importance and display\n",
        "feature_importance_data.sort(key=lambda x: x['IMPORTANCE_SCORE'], reverse=True)\n",
        "print(\"=== Feature Importance (XGBoost) ===\")\n",
        "for fi in feature_importance_data[:5]:\n",
        "    print(f\"  {fi['FEATURE_NAME']:25s} {fi['IMPORTANCE_SCORE']:.4f} ({fi['FEATURE_TYPE']})\")\n",
        "\n",
        "# Save to table\n",
        "fi_df = session.create_dataframe(feature_importance_data)\n",
        "fi_df.write.mode(\"overwrite\").save_as_table(\"ATOMIC.MODEL_FEATURE_IMPORTANCE\")\n",
        "print(f\"\\nFeature importance saved to ATOMIC.MODEL_FEATURE_IMPORTANCE ({len(feature_importance_data)} features)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c9401c2",
      "metadata": {
        "language": "python",
        "name": "model_evaluation_cell"
      },
      "outputs": [],
      "source": [
        "# Evaluate all models and compare performance\n",
        "print(\"=== Multi-Model Performance Comparison ===\\n\")\n",
        "\n",
        "# Define training parameters for each model (for registry tracking)\n",
        "training_params = {\n",
        "    'XGBoost': {\n",
        "        'n_estimators': 100,\n",
        "        'max_depth': 6,\n",
        "        'learning_rate': 0.1,\n",
        "        'random_state': 42\n",
        "    },\n",
        "    'RandomForest': {\n",
        "        'n_estimators': 100,\n",
        "        'max_depth': 8,\n",
        "        'random_state': 42\n",
        "    },\n",
        "    'LinearRegression': {}\n",
        "}\n",
        "\n",
        "model_results = []\n",
        "best_model = None\n",
        "best_mape = float('inf')\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    # Generate predictions\n",
        "    predictions_df = model.predict(test_df)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    metrics = predictions_df.select(\n",
        "        F.avg(F.abs(F.col(\"PREDICTION\") - F.col(target_col))).alias(\"MAE\"),\n",
        "        F.sqrt(F.avg(F.pow(F.col(\"PREDICTION\") - F.col(target_col), 2))).alias(\"RMSE\"),\n",
        "        F.avg(F.abs((F.col(\"PREDICTION\") - F.col(target_col)) / F.col(target_col)) * 100).alias(\"MAPE\"),\n",
        "        F.corr(F.col(\"PREDICTION\"), F.col(target_col)).alias(\"CORR\")\n",
        "    ).collect()[0]\n",
        "    \n",
        "    mae = float(metrics['MAE'])\n",
        "    rmse = float(metrics['RMSE'])\n",
        "    mape = float(metrics['MAPE'])\n",
        "    corr = float(metrics['CORR']) if metrics['CORR'] else 0\n",
        "    r2 = corr ** 2  # Approximate R²\n",
        "    \n",
        "    print(f\"{model_name}:\")\n",
        "    print(f\"  MAE:  {mae:.2f}\")\n",
        "    print(f\"  RMSE: {rmse:.2f}\")\n",
        "    print(f\"  MAPE: {mape:.2f}%\")\n",
        "    print(f\"  R²:   {r2:.4f}\")\n",
        "    print()\n",
        "    \n",
        "    # Track best model\n",
        "    if mape < best_mape:\n",
        "        best_mape = mape\n",
        "        best_model = model_name\n",
        "    \n",
        "    model_results.append({\n",
        "        'MODEL_NAME': f'Demand Sensing {model_name}',\n",
        "        'MODEL_VERSION': 'v5.0.0',\n",
        "        'ALGORITHM': model_name,\n",
        "        'MAE': mae,\n",
        "        'RMSE': rmse,\n",
        "        'MAPE': mape,\n",
        "        'R2_SCORE': r2,\n",
        "        'FEATURE_COUNT': len(feature_cols),\n",
        "        'TRAINING_SAMPLES': train_df.count(),\n",
        "        'IS_DEPLOYED': (model_name == 'XGBoost'),  # Deploy XGBoost by default\n",
        "        'TRAINING_PARAMETERS': training_params.get(model_name, {})\n",
        "    })\n",
        "\n",
        "print(f\"Best Model: {best_model} (MAPE: {best_mape:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f600c2a",
      "metadata": {
        "language": "python",
        "name": "model_evaluation_visualization"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MODEL EVALUATION VISUALIZATIONS\n",
        "# =============================================================================\n",
        "# Visualize model performance comparison\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Create DataFrame from model results for plotting\n",
        "results_df = pd.DataFrame(model_results)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "# Plot 1: Model Performance Comparison (MAPE - lower is better)\n",
        "ax1 = axes[0]\n",
        "models_list = results_df['ALGORITHM'].tolist()\n",
        "mape_values = results_df['MAPE'].tolist()\n",
        "bars = ax1.bar(models_list, mape_values, color=[COLORS[0], COLORS[1], COLORS[2]])\n",
        "ax1.set_ylabel('MAPE (%)')\n",
        "ax1.set_title('Model Comparison: MAPE (Lower is Better)')\n",
        "ax1.axhline(y=min(mape_values), color=COLORS[3], linestyle='--', \n",
        "            label=f'Best: {min(mape_values):.2f}%', alpha=0.7)\n",
        "ax1.legend(loc='upper right')\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, val in zip(bars, mape_values):\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
        "             f'{val:.1f}%', ha='center', va='bottom', fontsize=10, color='#E5E5E7')\n",
        "\n",
        "# Plot 2: R² Score Comparison (higher is better)\n",
        "ax2 = axes[1]\n",
        "r2_values = results_df['R2_SCORE'].tolist()\n",
        "bars = ax2.bar(models_list, r2_values, color=[COLORS[0], COLORS[1], COLORS[2]])\n",
        "ax2.set_ylabel('R² Score')\n",
        "ax2.set_title('Model Comparison: R² (Higher is Better)')\n",
        "ax2.axhline(y=max(r2_values), color=COLORS[3], linestyle='--', \n",
        "            label=f'Best: {max(r2_values):.4f}', alpha=0.7)\n",
        "ax2.legend(loc='lower right')\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Plot 3: MAE and RMSE Comparison\n",
        "ax3 = axes[2]\n",
        "x = np.arange(len(models_list))\n",
        "width = 0.35\n",
        "mae_values = results_df['MAE'].tolist()\n",
        "rmse_values = results_df['RMSE'].tolist()\n",
        "\n",
        "bars1 = ax3.bar(x - width/2, mae_values, width, label='MAE', color=COLORS[0])\n",
        "bars2 = ax3.bar(x + width/2, rmse_values, width, label='RMSE', color=COLORS[1])\n",
        "ax3.set_ylabel('Error (units)')\n",
        "ax3.set_title('MAE vs RMSE by Model')\n",
        "ax3.set_xticks(x)\n",
        "ax3.set_xticklabels(models_list)\n",
        "ax3.legend()\n",
        "ax3.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/tmp/model_comparison.png', dpi=150, bbox_inches='tight',\n",
        "            facecolor='#121212', edgecolor='none')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nBest performing model: {best_model}\")\n",
        "print(f\"Model comparison visualizations saved to /tmp/model_comparison.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71b2cd09",
      "metadata": {
        "name": "model_registry_header"
      },
      "source": [
        "## 7. Production Output\n",
        "\n",
        "Save model performance metrics to `MODEL_REGISTRY` for tracking, generate 90-day forecasts + 6-month backtests, and persist to `DEMAND_FORECAST_PREDICTIONS`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b517a512",
      "metadata": {
        "language": "python",
        "name": "save_registry_and_forecast_cell"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SAVE TO MODEL REGISTRY\n",
        "# =============================================================================\n",
        "# Persist model metrics with training metadata\n",
        "# Table schema: MODEL_ID, MODEL_NAME, MODEL_VERSION, ALGORITHM, TRAINING_DATE,\n",
        "#               MAE, RMSE, MAPE, R2_SCORE, FEATURE_COUNT, TRAINING_SAMPLES,\n",
        "#               IS_DEPLOYED, DEPLOYMENT_DATE, MODEL_ARTIFACT_PATH,\n",
        "#               TRAINING_PARAMETERS, CREATED_TIMESTAMP\n",
        "\n",
        "import json\n",
        "\n",
        "# Get max MODEL_ID from registry to generate new IDs\n",
        "max_id_result = session.sql(\"SELECT COALESCE(MAX(MODEL_ID), 0) AS MAX_ID FROM ATOMIC.MODEL_REGISTRY\").collect()[0]\n",
        "next_model_id = int(max_id_result['MAX_ID']) + 1\n",
        "\n",
        "# Build rows in exact column order matching table schema\n",
        "current_timestamp = datetime.datetime.now()\n",
        "registry_rows = []\n",
        "for idx, result in enumerate(model_results):\n",
        "    row = (\n",
        "        next_model_id + idx,                                                           # MODEL_ID\n",
        "        result['MODEL_NAME'],                                                          # MODEL_NAME\n",
        "        result['MODEL_VERSION'],                                                       # MODEL_VERSION\n",
        "        result['ALGORITHM'],                                                           # ALGORITHM\n",
        "        current_timestamp,                                                             # TRAINING_DATE\n",
        "        float(result['MAE']),                                                          # MAE\n",
        "        float(result['RMSE']),                                                         # RMSE\n",
        "        float(result['MAPE']),                                                         # MAPE\n",
        "        float(result['R2_SCORE']),                                                     # R2_SCORE\n",
        "        int(result['FEATURE_COUNT']),                                                  # FEATURE_COUNT\n",
        "        int(result['TRAINING_SAMPLES']),                                               # TRAINING_SAMPLES\n",
        "        result['IS_DEPLOYED'],                                                         # IS_DEPLOYED\n",
        "        current_timestamp if result['IS_DEPLOYED'] else None,                          # DEPLOYMENT_DATE\n",
        "        f\"snowflake://models/demand_sensing/{result['MODEL_VERSION']}/{result['ALGORITHM'].lower()}\",  # MODEL_ARTIFACT_PATH\n",
        "        result.get('TRAINING_PARAMETERS', {}),                                         # TRAINING_PARAMETERS\n",
        "        current_timestamp                                                              # CREATED_TIMESTAMP\n",
        "    )\n",
        "    registry_rows.append(row)\n",
        "\n",
        "# Define explicit schema matching table DDL\n",
        "registry_schema = StructType([\n",
        "    StructField(\"MODEL_ID\", LongType(), nullable=False),\n",
        "    StructField(\"MODEL_NAME\", StringType(), nullable=False),\n",
        "    StructField(\"MODEL_VERSION\", StringType(), nullable=False),\n",
        "    StructField(\"ALGORITHM\", StringType(), nullable=True),\n",
        "    StructField(\"TRAINING_DATE\", TimestampType(), nullable=True),\n",
        "    StructField(\"MAE\", FloatType(), nullable=True),\n",
        "    StructField(\"RMSE\", FloatType(), nullable=True),\n",
        "    StructField(\"MAPE\", FloatType(), nullable=True),\n",
        "    StructField(\"R2_SCORE\", FloatType(), nullable=True),\n",
        "    StructField(\"FEATURE_COUNT\", IntegerType(), nullable=True),\n",
        "    StructField(\"TRAINING_SAMPLES\", LongType(), nullable=True),\n",
        "    StructField(\"IS_DEPLOYED\", BooleanType(), nullable=True),\n",
        "    StructField(\"DEPLOYMENT_DATE\", TimestampType(), nullable=True),\n",
        "    StructField(\"MODEL_ARTIFACT_PATH\", StringType(), nullable=True),\n",
        "    StructField(\"TRAINING_PARAMETERS\", VariantType(), nullable=True),\n",
        "    StructField(\"CREATED_TIMESTAMP\", TimestampType(), nullable=True)\n",
        "])\n",
        "\n",
        "registry_df = session.create_dataframe(registry_rows, schema=registry_schema)\n",
        "\n",
        "# Write to registry with fail-fast error handling\n",
        "registry_df.write.mode(\"append\").save_as_table(\"ATOMIC.MODEL_REGISTRY\")\n",
        "print(f\"Saved {len(model_results)} models to ATOMIC.MODEL_REGISTRY\")\n",
        "\n",
        "# Verify write succeeded\n",
        "verify_count = session.sql(\"SELECT COUNT(*) AS CNT FROM ATOMIC.MODEL_REGISTRY WHERE MODEL_VERSION = 'v5.0.0'\").collect()[0]['CNT']\n",
        "if verify_count < len(model_results):\n",
        "    raise RuntimeError(f\"Registry write verification failed: expected {len(model_results)}, found {verify_count}\")\n",
        "print(f\"Verified {verify_count} v5.0.0 models in registry\")\n",
        "\n",
        "# Show registry contents\n",
        "print(\"\\n=== Model Registry Summary ===\")\n",
        "session.sql(\"\"\"\n",
        "    SELECT MODEL_NAME, MODEL_VERSION, ALGORITHM, MAPE, R2_SCORE, IS_DEPLOYED\n",
        "    FROM ATOMIC.MODEL_REGISTRY\n",
        "    WHERE MODEL_VERSION = 'v5.0.0'\n",
        "    ORDER BY MAPE ASC\n",
        "\"\"\").show()\n",
        "\n",
        "# =============================================================================\n",
        "# GENERATE BACKTESTS + 90-DAY FORECASTS\n",
        "# =============================================================================\n",
        "# Backtests: Last 6 months of historical data (for accuracy metrics)\n",
        "# Forecasts: Next 90 days from today (for planning)\n",
        "\n",
        "print(\"\\n=== Generating Backtests + 90-Day Forecasts ===\")\n",
        "\n",
        "forecast_sql = \"\"\"\n",
        "WITH demand_stats AS (\n",
        "    SELECT \n",
        "        PRODUCT_ID,\n",
        "        SITE_ID,\n",
        "        AVG(ACTUAL_QUANTITY) AS AVG_QTY,\n",
        "        STDDEV(ACTUAL_QUANTITY) AS STD_QTY\n",
        "    FROM ATOMIC.DEMAND_ACTUAL\n",
        "    GROUP BY PRODUCT_ID, SITE_ID\n",
        "),\n",
        "-- Get the date range of actual demand data for backtesting\n",
        "actual_date_range AS (\n",
        "    SELECT \n",
        "        MAX(DEMAND_DATE) AS MAX_ACTUAL_DATE,\n",
        "        DATEADD(month, -6, MAX(DEMAND_DATE)) AS BACKTEST_START\n",
        "    FROM ATOMIC.DEMAND_ACTUAL\n",
        "),\n",
        "-- BACKTEST dates: last 6 months of historical data where we have actuals\n",
        "backtest_dates AS (\n",
        "    SELECT DISTINCT da.DEMAND_DATE AS FORECAST_DATE\n",
        "    FROM ATOMIC.DEMAND_ACTUAL da\n",
        "    CROSS JOIN actual_date_range adr\n",
        "    WHERE da.DEMAND_DATE >= adr.BACKTEST_START\n",
        "),\n",
        "-- FORECAST dates: next 90 days from today (future predictions)\n",
        "forecast_dates AS (\n",
        "    SELECT DATEADD(day, seq4(), CURRENT_DATE()) AS FORECAST_DATE \n",
        "    FROM TABLE(GENERATOR(ROWCOUNT => 90))\n",
        "),\n",
        "-- Union both backtest and forecast dates\n",
        "all_dates AS (\n",
        "    SELECT FORECAST_DATE FROM backtest_dates\n",
        "    UNION\n",
        "    SELECT FORECAST_DATE FROM forecast_dates\n",
        "),\n",
        "combos AS (SELECT DISTINCT PRODUCT_ID, SITE_ID FROM demand_stats LIMIT 100),\n",
        "-- External indicators - use historical for backtests, latest for forecasts\n",
        "external_indicators AS (\n",
        "    SELECT \n",
        "        DATE_TRUNC('week', INDICATOR_DATE) AS WEEK_DATE,\n",
        "        AVG(CASE WHEN INDICATOR_NAME = 'Construction Starts Index' THEN INDICATOR_VALUE END) AS CONSTRUCTION_IDX,\n",
        "        AVG(CASE WHEN INDICATOR_NAME = 'Clinical Trial Spend ($B)' THEN INDICATOR_VALUE END) AS CLINICAL_IDX,\n",
        "        AVG(CASE WHEN INDICATOR_NAME = 'Industrial Production Index' THEN INDICATOR_VALUE END) AS PRODUCTION_IDX,\n",
        "        AVG(CASE WHEN INDICATOR_NAME = 'Manufacturing PMI' THEN INDICATOR_VALUE END) AS PMI_IDX\n",
        "    FROM ATOMIC.MARKETPLACE_INDICATORS\n",
        "    GROUP BY DATE_TRUNC('week', INDICATOR_DATE)\n",
        "),\n",
        "external_latest AS (\n",
        "    SELECT \n",
        "        AVG(CONSTRUCTION_IDX) AS CONSTRUCTION_IDX,\n",
        "        AVG(CLINICAL_IDX) AS CLINICAL_IDX,\n",
        "        AVG(PRODUCTION_IDX) AS PRODUCTION_IDX,\n",
        "        AVG(PMI_IDX) AS PMI_IDX\n",
        "    FROM external_indicators\n",
        "    WHERE WEEK_DATE = (SELECT MAX(WEEK_DATE) FROM external_indicators)\n",
        ")\n",
        "SELECT \n",
        "    c.PRODUCT_ID, c.SITE_ID, d.FORECAST_DATE AS PREDICTION_DATE,\n",
        "    CAST(COALESCE(ds.AVG_QTY, 100) AS FLOAT) AS HIST_AVG_QTY,\n",
        "    CAST(COALESCE(ds.STD_QTY, 10) AS FLOAT) AS HIST_STD_QTY,\n",
        "    CAST(COALESCE(p.PRODUCT_CATEGORY_ID, 1) AS FLOAT) AS PRODUCT_CATEGORY_ID,\n",
        "    -- Use historical indicators for backtests, latest for future forecasts\n",
        "    CAST(COALESCE(ei.CONSTRUCTION_IDX, el.CONSTRUCTION_IDX, 100) AS FLOAT) AS CONSTRUCTION_IDX,\n",
        "    CAST(COALESCE(ei.CLINICAL_IDX, el.CLINICAL_IDX, 2.5) AS FLOAT) AS CLINICAL_IDX,\n",
        "    CAST(COALESCE(ei.PRODUCTION_IDX, el.PRODUCTION_IDX, 98) AS FLOAT) AS PRODUCTION_IDX,\n",
        "    CAST(COALESCE(ei.PMI_IDX, el.PMI_IDX, 50) AS FLOAT) AS PMI_IDX,\n",
        "    CAST(DAYOFWEEK(d.FORECAST_DATE) AS FLOAT) AS DAY_OF_WEEK,\n",
        "    CAST(MONTH(d.FORECAST_DATE) AS FLOAT) AS MONTH_NUM,\n",
        "    CAST(QUARTER(d.FORECAST_DATE) AS FLOAT) AS QUARTER_NUM\n",
        "FROM combos c \n",
        "CROSS JOIN all_dates d\n",
        "CROSS JOIN external_latest el\n",
        "LEFT JOIN demand_stats ds ON c.PRODUCT_ID = ds.PRODUCT_ID AND c.SITE_ID = ds.SITE_ID\n",
        "LEFT JOIN ATOMIC.PRODUCT p ON c.PRODUCT_ID = p.PRODUCT_ID\n",
        "LEFT JOIN external_indicators ei ON DATE_TRUNC('week', d.FORECAST_DATE) = ei.WEEK_DATE\n",
        "\"\"\"\n",
        "\n",
        "forecast_input = session.sql(forecast_sql)\n",
        "forecast_output = xgb_model.predict(forecast_input)\n",
        "\n",
        "# Calculate confidence intervals (90%)\n",
        "# Using historical std deviation for interval estimation\n",
        "output_df = forecast_output.select(\n",
        "    F.col(\"PRODUCT_ID\"), \n",
        "    F.col(\"SITE_ID\"), \n",
        "    F.col(\"PREDICTION_DATE\"),\n",
        "    F.col(\"PREDICTION\").alias(\"FORECASTED_MATERIAL_DEMAND_QTY\"),\n",
        "    # 90% confidence interval (+/- 1.645 * std estimate)\n",
        "    F.greatest(F.lit(0), F.col(\"PREDICTION\") - F.col(\"HIST_STD_QTY\") * 1.645).alias(\"PREDICTION_LOWER_BOUND\"),\n",
        "    (F.col(\"PREDICTION\") + F.col(\"HIST_STD_QTY\") * 1.645).alias(\"PREDICTION_UPPER_BOUND\"),\n",
        "    F.lit(0.90).alias(\"CONFIDENCE_LEVEL\"),\n",
        "    F.lit(\"v5.0.0\").alias(\"MODEL_VERSION\"),\n",
        "    F.current_timestamp().alias(\"CREATED_TIMESTAMP\")\n",
        ")\n",
        "\n",
        "# Write to table\n",
        "output_df.write.mode(\"overwrite\").save_as_table(\"ATOMIC.DEMAND_FORECAST_PREDICTIONS\")\n",
        "forecast_count = output_df.count()\n",
        "\n",
        "# Verify write and count backtests vs forecasts\n",
        "verify_forecast = session.sql(\"SELECT COUNT(*) AS CNT FROM ATOMIC.DEMAND_FORECAST_PREDICTIONS\").collect()[0]['CNT']\n",
        "if verify_forecast != forecast_count:\n",
        "    raise RuntimeError(f\"Forecast write verification failed: wrote {forecast_count}, found {verify_forecast}\")\n",
        "\n",
        "backtest_count = session.sql(\"\"\"\n",
        "    SELECT COUNT(*) AS CNT FROM ATOMIC.DEMAND_FORECAST_PREDICTIONS \n",
        "    WHERE PREDICTION_DATE <= (SELECT MAX(DEMAND_DATE) FROM ATOMIC.DEMAND_ACTUAL)\n",
        "\"\"\").collect()[0]['CNT']\n",
        "future_count = forecast_count - backtest_count\n",
        "\n",
        "print(f\"\\n{forecast_count:,} total predictions written to ATOMIC.DEMAND_FORECAST_PREDICTIONS\")\n",
        "print(f\"  - Backtests (historical): {backtest_count:,} (for accuracy metrics)\")\n",
        "print(f\"  - Forecasts (future): {future_count:,} (for planning)\")\n",
        "print(f\"  Model: XGBoost v5.0.0\")\n",
        "print(f\"  Confidence Level: 90%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b52bee5",
      "metadata": {
        "name": "key_takeaways"
      },
      "source": [
        "## 8. Key Takeaways & Interpretation Guide\n",
        "\n",
        "### What the Model Learned\n",
        "\n",
        "1. **Historical patterns dominate**: `HIST_AVG_QTY` is typically the strongest predictor, indicating demand tends to follow historical averages\n",
        "2. **Seasonal effects matter**: Day of week and month contribute to predictions, capturing weekly/monthly demand cycles\n",
        "3. **External indicators add value**: Macro-economic signals like PMI and construction starts improve forecasting accuracy\n",
        "\n",
        "### Interpretation Guidelines\n",
        "\n",
        "| Metric | Good Value | Interpretation |\n",
        "|--------|------------|----------------|\n",
        "| **MAPE** | < 20% | Predictions within 20% of actuals on average |\n",
        "| **R²** | > 0.7 | Model explains >70% of demand variance |\n",
        "| **MAE** | Context-dependent | Average absolute error in demand units |\n",
        "| **RMSE** | < 1.5× MAE | Low RMSE/MAE ratio indicates few large errors |\n",
        "\n",
        "### Confidence Intervals\n",
        "\n",
        "- **90% Confidence**: `PREDICTION_LOWER_BOUND` to `PREDICTION_UPPER_BOUND`\n",
        "- **Interpretation**: We expect 90% of actual values to fall within this range\n",
        "- **Formula**: $\\hat{y} \\pm 1.645 \\times \\sigma_{historical}$\n",
        "\n",
        "### Limitations & Considerations\n",
        "\n",
        "1. **Assumes stationarity**: Model may not adapt quickly to structural changes in demand patterns\n",
        "2. **External data lag**: Macro-economic indicators may be published with delay\n",
        "3. **Limited to 90-day horizon**: Forecast accuracy degrades beyond this window\n",
        "4. **No supply constraints**: Model predicts demand, not what can be fulfilled\n",
        "\n",
        "### Mathematical Recap\n",
        "\n",
        "**XGBoost Objective (simplified)**:\n",
        "$$L(\\theta) = \\sum_i l(y_i, \\hat{y}_i) + \\sum_k \\Omega(f_k)$$\n",
        "\n",
        "Where $l$ is the loss function (MSE for regression) and $\\Omega$ is the regularization term preventing overfitting.\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Monitor forecast accuracy**: Compare backtests to actuals weekly\n",
        "2. **Retrain periodically**: Update model monthly with new data\n",
        "3. **Expand features**: Consider adding supplier lead times, promotions, weather\n",
        "4. **Alert thresholds**: Set up alerts when MAPE exceeds acceptable levels"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
